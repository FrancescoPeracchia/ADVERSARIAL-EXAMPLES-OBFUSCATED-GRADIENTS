# ADVERSARIAL-EXAMPLES-OBFUSCATED-GRADIENTS
Adversarial examples are inputs to machine learning models that an attacker has intentionally
designed to cause the model to make a mistake; they’re like optical illusions for machines.
A defense is said to cause gradient masking if it “does not have useful gradients” for generating
adversarial examples.

Here is presented a paper for an introductory approach to this topic, and a possible way to build a defence for adversarial examples.

link for prezi presentation: https://prezi.com/view/ubIs71IKizhVBGvNP7hh/
